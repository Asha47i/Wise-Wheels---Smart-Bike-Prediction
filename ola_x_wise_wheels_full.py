# -*- coding: utf-8 -*-
"""Ola x Wise Wheels full

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hy-tOxoR-DezIzN4kLzhvNKrr90pnDdV

# **OLA X WISE WHEELS**

The goal of this project is to understand and predict bike rental patterns in Seoul using the SeoulBike dataset. By analyzing how different factors such as weather, time of day, and holidays—affect bike rentals, we can derive insights to help businesses optimize bike availability and improve customer experience.

# **1. Imports and Setup**
We start by importing the necessary libraries:

- Pandas, NumPy for data handling

- Matplotlib, Seaborn for visuals (optional)

- Scikit-learn for clustering (KMeans), scaling (StandardScaler), and evaluation (MSE, R²)
"""

#imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, r2_score
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.spatial.distance import cdist
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from xgboost import XGBRegressor

"""# **2. Loading and Preparing the Data**
- We load the SeoulBike.csv dataset and start with feature engineering to make the data richer:

    - Extract Hour, Day, Month, Year, Weekday from the Date.
    - Create new features like IsWeekend, WorkingDay, and PeakHour to capture user behavior better.
    - Label encode categorical features like Seasons, Holiday, and Functioning Day for model compatibility.
    - Drop Date since it's no longer needed.

"""

#Load dataset
df = pd.read_csv('/content/SeoulBike.csv')

df.info()

df.isna().sum()

"""# **Data Profiling**
Used ydata_profiling to generate an automated EDA report, providing an overview of distributions, correlations, and missingness.
"""

#!pip install ydata_profiling
#from ydata_profiling import ProfileReport

#profile= ProfileReport(df, title='BikeData')
#profile.to_notebook_iframe()

"""# **Feature Engineering**
- Extracted new features: Day, Month, Year, Weekday, and IsWeekend from the Date.
- These additional features helped capture temporal patterns more effectively.
"""

#feature Engineering
df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')
df['Hour'] = df['Hour'].astype(int)
df['Day'] = df['Date'].dt.day
df['Month'] = df['Date'].dt.month
df['Year'] = df['Date'].dt.year
df['Weekday'] = df['Date'].dt.weekday
df['IsWeekend'] = df['Weekday'].apply(lambda x: 1 if x >= 5 else 0)
df['Dayofweek'] = df['Date'].dt.dayofweek

#peak hour (rush hours 7-9am and 5-7pm)
df['PeakHour'] = df['Hour'].apply(lambda x: 1 if (7 <= x <= 9) or (17 <= x <= 19) else 0)

#working Day (1 if Monday-Friday)
df['WorkingDay'] = df['Dayofweek'].apply(lambda x: 1 if x < 5 else 0)

"""# **Exploratory Data Analysis (EDA)**
- Visualized rental trends by hour using a line plot.
- Investigated the relationship between temperature and bike rentals with a scatterplot.
- Compared rental averages across seasons and holidays using bar plots.
- Generated a correlation heatmap to better understand feature relationships.
"""

#trend by hour
sns.lineplot(data=df, x='Hour', y='Rented Bike Count', color='salmon',estimator='mean')
plt.title('Average Bikes Rented by Hour')
plt.xlabel('Hour of Day')
plt.ylabel('Average Rented Bike Count')
plt.xticks(range(0, 24))
plt.show()

"""Rentals peak during morning (7-8 am) and evening commute hours (5-6 pm)"""

#trend by temperature and bike rentals (Scatterplot)
sns.scatterplot(data=df, x='Temperature(°C)', y='Rented Bike Count',color='salmon', alpha=0.5)
plt.title('Bike Rentals vs Temperature')
plt.xlabel('Temperature (°C)')
plt.ylabel('Rented Bike Count')
plt.show()

"""Moderate temperatures have higher rental activity."""

#rentals across seasons
plt.figure(figsize=(10, 6))
sns.barplot(data=df, x='Seasons', y='Rented Bike Count',color='salmon', errorbar=None)
plt.title('Average Bike Rentals by Season')
plt.xlabel('Season')
plt.ylabel('Average Rented Bike Count')
plt.show()

"""Spring and Autumn have higher usage than Winter.

"""

# Ride count by month
plt.figure(figsize=(10,6))
sns.barplot(x='Month', y='Rented Bike Count', data=df,color='salmon',errorbar=None)
plt.title('Average Rentals by Month')
plt.show()

"""Bike rentals peak sharply in June and remain high during the summer months, while winter months (especially January, February, and December) see the lowest rental activity."""

# Ride count by weekday
df['Weekday'] = df['Date'].dt.dayofweek
plt.figure(figsize=(10,6))
sns.barplot(x='Weekday', y='Rented Bike Count', data=df,color='salmon',errorbar=None)
plt.title('Average Rentals by Weekday')
plt.show()

"""Bike rentals are fairly consistent across weekdays, but slightly higher on Monday, Wednesday, and Thursday. Rentals noticeably drop on Sunday, suggesting reduced commuting or travel activity."""

# Assuming df is your DataFrame
plt.figure(figsize=(12, 9))
sns.heatmap(
    df.corr(numeric_only=True),
    annot=True,
    cmap="Reds",  # Corrected colormap to 'Reds'
    linewidths=0.5,
    linecolor='red'
)
plt.title('Correlation Heatmap', fontsize=16)
plt.show()

"""Temperature, humidity, and wind speed have noticeable effects.

"""

#Holiday vs Non-Holiday
plt.figure(figsize=(8, 5))
sns.barplot(data=df, x='Holiday', y='Rented Bike Count', color='salmon',errorbar=None)
plt.title('Average Bike Rentals: Holiday vs Non-Holiday', fontsize=14)
plt.xlabel('Is it a Holiday?', fontsize=12)
plt.ylabel('Average Rented Bike Count', fontsize=12)
plt.xticks(fontsize=11)
plt.yticks(fontsize=11)
plt.tight_layout()
plt.show()

"""Fewer rentals happen on holidays."""

#bike rentals on Weekends vs Weekdays
plt.figure(figsize=(8,6))
sns.barplot(data=df, x='IsWeekend', y='Rented Bike Count',color='salmon', errorbar=None)
plt.title('Average Bike Rentals: Weekends vs Weekdays')
plt.xlabel('Is Weekend (0 = Weekday, 1 = Weekend)')
plt.ylabel('Average Rented Bike Count')
plt.show()

"""Weekdays show higher demand, especially for commuting."""

#Encode categorical variables
label_encoders = {}
for col in ['Seasons', 'Holiday', 'Functioning Day']:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le

#Drop 'Date' if not needed later
df.drop(columns=['Date'], inplace=True)

"""# **Clustering Seoul Bike Dataset using KMeans & Evaluating with MSE and R²**

- The goal here is simple:
We want to better understand the bike rental patterns in Seoul based on different dimensions — weather, time, operations, and pure usage — without using any supervised learning.

- Instead of predicting rentals using a classic model like regression, we group similar instances together using KMeans clustering.
After grouping, we check how "good" or "coherent" these groups are by evaluating them using MSE (Mean Squared Error) and R² (coefficient of determination) — metrics usually used in supervised settings — but here adapted to measure cluster performance.

**NOTE :** *In the evaluation of the clustering model, the Mean Squared Error (MSE) and R² metrics are being applied to assess how well the clusters represent the Rented Bike Count. However, it's important to understand that the method used is a simplified approximation rather than a predictive model.*

# **Defining Feature Groups for Clustering**
We group features into logical clusters:
- Weather Features :Temperature, Humidity, Wind speed, etc.
- Temporal Features: Hour of the day, Day of week, Holiday info
- Operational Features: Hour + weather conditions + rental counts
- Bike Usage Features	Only Hour and Rental Count

Each group represents a different angle to analyze rental behavior — weather influence, time-based influence, operational behavior, and pure rental usage trends.
"""

# Assuming the DataFrame is already loaded as df
features_weather = [
    'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Visibility (10m)',
    'Dew point temperature(°C)', 'Solar Radiation (MJ/m2)', 'Rainfall(mm)', 'Snowfall (cm)'
]

features_temporal = ['Hour', 'Dayofweek', 'Holiday', 'IsWeekend', 'Seasons']

features_operational = ['Hour', 'Temperature(°C)', 'Humidity(%)', 'Wind speed (m/s)', 'Rented Bike Count']

features_bike_usage = ['Hour','Rented Bike Count']

# Scaling the features
scaler = StandardScaler()
X_weather = scaler.fit_transform(df[features_weather])
X_temporal = scaler.fit_transform(df[features_temporal])
X_operational = scaler.fit_transform(df[features_operational])
X_bike_usage = scaler.fit_transform(df[features_bike_usage])

"""# **Scaling the Data**
- We standardize features using StandardScaler:
KMeans is distance-based, so scaling ensures that no feature (e.g., temperature vs. rainfall) unfairly dominates the clustering.
"""

scaler_weather = StandardScaler()
X_weather_scaled = scaler_weather.fit_transform(X_weather)

scaler_temporal = StandardScaler()
X_temporal_scaled = scaler_temporal.fit_transform(X_temporal)

scaler_operational = StandardScaler()
X_operational_scaled = scaler_operational.fit_transform(X_operational)

scaler_bike_usage = StandardScaler()
X_bike_usage_scaled = scaler_bike_usage.fit_transform(X_bike_usage)

"""# **Clustering with KMeans**
We apply KMeans clustering with 4 clusters (n_clusters=4) for each feature group separately.

Each KMeans model assigns a cluster label to every record in the dataset.

It's a reasonable starting point for interpretability (e.g., High, Moderate, Low, Very Low patterns) — but tuning cluster numbers can be done later using methods like Elbow Method.
"""

# KMeans clustering
kmeans_weather = KMeans(n_clusters=4, random_state=42)
df['Weather_Cluster'] = kmeans_weather.fit_predict(X_weather)

kmeans_temporal = KMeans(n_clusters=4, random_state=42)
df['Temporal_Cluster'] = kmeans_temporal.fit_predict(X_temporal)

kmeans_operational = KMeans(n_clusters=4, random_state=42)
df['Operational_Cluster'] = kmeans_operational.fit_predict(X_operational)

kmeans_bike_usage = KMeans(n_clusters=4, random_state=42)
df['Bike_Usage_Cluster'] = kmeans_bike_usage.fit_predict(X_bike_usage)

"""# **Mapping Cluster Labels to Meaningful Profiles**
After clustering, we manually map the numeric cluster labels (0, 1, 2, 3) into more understandable "profiles".

Examples:

- Weather Cluster 0 → "Sunny-High Rental"

- Temporal Cluster 1 → "Weekend-Moderate Rentals"

- Bike Usage Cluster 3 → "Very Low Usage Hours"

*Makes results more human-readable and actionable, especially for business decisions.*


"""

# Assuming df is already your DataFrame with the clusters
# Mapping cluster labels to more descriptive weather profiles
df['Weather_Profile'] = df['Weather_Cluster'].map({
    0: 'Sunny-High Rental',
    1: 'Rainy-Low Rental',
    2: 'Cloudy-Moderate Rental',
    3: 'Moderate Rentals'
})

# Mapping cluster labels to temporal clusters (e.g., weekend vs weekday)
df['Temporal_Profile'] = df['Temporal_Cluster'].map({
    0: 'Weekday-High Rentals',
    1: 'Weekend-Moderate Rentals',
    2: 'Holiday-Low Rentals',
    3: 'Weekday-Low Rentals'
})

# Mapping cluster labels to operational profiles (if you have them based on operational data)
df['Operational_Profile'] = df['Operational_Cluster'].map({
    0: 'Peak Hours-High Rentals',
    1: 'Off-Peak-Moderate Rentals',
    2: 'Maintenance-Low Rentals',
    3: 'Standard Rentals'
})

# Mapping cluster labels to bike usage patterns (e.g., high usage hours, low usage hours)
df['Bike_Usage_Profile'] = df['Bike_Usage_Cluster'].map({
    0: 'High Usage Hours',
    1: 'Moderate Usage Hours',
    2: 'Low Usage Hours',
    3: 'Very Low Usage Hours'
})

# View the resulting DataFrame with the new descriptive profiles
print(df[['Weather_Profile', 'Temporal_Profile', 'Operational_Profile', 'Bike_Usage_Profile']].head())

"""# **Evaluating Cluster Quality: MSE and R²**
- Since we don't have "true labels," we creatively repurpose regression metrics:

  - For each cluster, take the average "Rented Bike Count" within that cluster.

  - Predict each instance's rentals using its cluster's average rental.

  - Compare the predicted rental count vs. actual rental count using:

    - MSE (Lower is better — closer to real rentals)

    - R² (Closer to 1 is better — how much variance is explained)

Even though clustering is unsupervised, if clusters are meaningful, the average behavior within them should represent individual instances well.
"""

# Function to evaluate each cluster's performance (MSE, R²)
def evaluate_cluster_performance(df, cluster_column, target_column):
    # Get the predicted cluster labels
    cluster_labels = df[cluster_column]

    # For each cluster, assign the cluster's average rental count
    cluster_avg_rentals = df.groupby(cluster_column)[target_column].mean()

    # Predict: for each point, predict its cluster's average rental
    predicted_rentals = cluster_labels.map(cluster_avg_rentals)

    # Calculate MSE and R²
    mse = mean_squared_error(df[target_column], predicted_rentals)
    r2 = r2_score(df[target_column], predicted_rentals)

    return mse, r2

# Evaluate each cluster group
mse_weather, r2_weather = evaluate_cluster_performance(df, 'Weather_Cluster', 'Rented Bike Count')
mse_temporal, r2_temporal = evaluate_cluster_performance(df, 'Temporal_Cluster', 'Rented Bike Count')
mse_operational, r2_operational = evaluate_cluster_performance(df, 'Operational_Cluster', 'Rented Bike Count')
mse_bike_usage, r2_bike_usage = evaluate_cluster_performance(df, 'Bike_Usage_Cluster', 'Rented Bike Count')

# Print the results
print(f'Weather Clusters - MSE: {mse_weather:.2f}, R²: {r2_weather:.2f}')
print(f'Temporal Clusters - MSE: {mse_temporal:.2f}, R²: {r2_temporal:.2f}')
print(f'Operational Clusters - MSE: {mse_operational:.2f}, R²: {r2_operational:.2f}')
print(f'Bike Usage Clusters - MSE: {mse_bike_usage:.2f}, R²: {r2_bike_usage:.2f}')

"""Clustering based on bike usage patterns gave us the most meaningful segmentation, while temporal and weather clustering were less effective predictors of rentals.

# **Evaluating Various Modeling Approaches: Random Forest**
In this section, we shifted our focus to predicting bike rental counts using a Random Forest Regressor. After completing data loading, cleaning, and exploration — and building initial clusters with KMeans — we trained a Random Forest model. We evaluated its performance, examined feature importance, and compared its results with those from an XGBoost model.
"""
